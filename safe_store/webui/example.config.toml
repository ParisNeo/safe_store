# Default config.toml

[lollms]
binding_name = "ollama" # Examples: "ollama", "lollms", "openai"
host_address = "http://localhost:11434" # e.g., "http://localhost:9600" for lollms, null for openai
model_name = "mistral:latest" # e.g., "mistral:latest", "gpt-4", specific model path for lollms
# service_key = "sk-your_openai_key_here" # Only if needed, e.g. for OpenAI if not using env var

[safestore]
# General SafeStore settings (applied to all databases)
default_vectorizer = "st:all-MiniLM-L6-v2"
chunk_size = 10000
chunk_overlap = 100

[graphstore]
# Entity fusion settings
[graphstore.fusion]
enabled = true # Set to true to have the LLM try to merge similar entities, false to disable.

# Prompt templates can be very long, consider if they should be in config or loaded from separate files
# For now, let's assume GraphStore uses its internal defaults if these are not specified or are empty.
# graph_extraction_prompt_template_file = "prompts/graph_extraction.txt" 
# query_parsing_prompt_template_file = "prompts/query_parsing.txt"

[webui]
host = "localhost"
port = 9643
temp_upload_dir = "temp_uploaded_files_webui"
log_level = "INFO" # DEBUG, INFO, WARNING, ERROR, CRITICAL
# The 'name' of the database to load on startup. Must match one of the [[databases]] entries.
active_database_name = "default"


# --- Database Configurations ---
# Database file paths are now standardized and managed by the application.
# The backend will create these directories and files based on the 'name'.
# You only need to define the database names here.

[[databases]]
name = "default"
db_file = "databases/default/default.db"
doc_dir = "databases/default/docs"

# To add another database, you can let the UI create it for you, or
# you can manually add another block like this and restart:
# [[databases]]
# name = "project_alpha"
# db_file = "databases/project_alpha/project_alpha.db"
# doc_dir = "databases/project_alpha/docs"